{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Gradient Boosting Machine\n",
    "\n",
    "Primary Sources: [Tree Boosting with XGBoost](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf?sequence=1&isAllowed=y), [Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf), and [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do)\n",
    "\n",
    "The gradient boosting machine is an ensemble machine learning model that has risen to prominence in recent years due to exceptional performance on structured machine learning tasks. The gradient boosting machine not surprisingly is a boosting model, one of the two types of ensembles:\n",
    "\n",
    "* bagging (bootstrap aggregating): train individual learners independently and make a prediction by averaging individual predictions\n",
    "* boosting: train individual learners in sequence, with each individual learning from the mistakes of the previous. Predictions are made by weighting the predictions inversely proportional to the error of the individual. \n",
    "\n",
    "The modeling notebook showed examples of both ensembles: random forest and extra trees are bagging methods while AdaBoost (adaptive boosting) is a boosting ensemble. Whereas in adaptive boosting, the observations are re-weighted each iteration according to the magnitude of their residuals (with larger residuals receiving greater weight), in gradient boosting, the learners are trained directly on the residuals of the entire ensemble. Both methods are teaching each successive learner to focus on the errors of the previous learners, but using different methods. Furthermore, while adaptive boosting weights the individuals according to their performance on the training set, the contribution of each learner in gradient boosting is learned through an iterative gradient descent optimization method. The ultimate goal of machine learning is to approximate a function that maps the features to the targets, and gradient boosting can therefore be thought of as gradient descent in function space. \n",
    "\n",
    "Gradient Boosting is a general method that can be applied to any differentiable objective function and use any individual models (called weak learners). The most common weak learner is the decision tree leading to particular type of gradient boosting known as Gradient Boosted Regression Trees (GBRT). There are several open-source libraries for implementing Gradient Boosting in Python including [Scikit-Learn](http://scikit-learn.org/stable/index.html), [LightGBM](http://lightgbm.readthedocs.io/en/latest/), [XGBoost](https://xgboost.readthedocs.io/en/latest/), and [CatBoost](https://catboost.yandex/). Although Scikit-Learn is a go-to library for many machine learning algorithms, its version of the Gradient Boosting model is [less performant](https://datascience.stackexchange.com/questions/10943/why-is-xgboost-so-much-faster-than-sklearn-gradientboostingclassifier) with fewer customizations than the other options. In this notebook, we will implement a gradient boosting machine using the [LightGBM library](https://github.com/Microsoft/LightGBM) from Microsoft. This library includes a number of variations on the gradient boosting framework including [Dropout meets Multiple Addivive Regression Trees (DART)](https://arxiv.org/abs/1505.01866) and [Gradient-based One Sided Sampling (GOSS)](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf). We will stick to using the Gradient Boosting Regression Trees method. \n",
    "\n",
    "This notebook will cover some of the best practices for building a gradient boosting machine collected from multiple machine learning competitions and published research. We will put these practices to use by building a gradient boosting model step by step for the supervised regression building energy prediction problem. The end outcome is a function that will allow us to evaluate the gradient boosting machine alongside the other models developed in the [Modeling notebook](https://bitbucket.org/willkoehrsen/prediction-documentation/src/670f76b3c327aed7916cd2d73f3f84dec8d71c98/notebooks/Modeling.ipynb?at=master&fileviewer=file-view-default). These models will be tested on hundreds of building datasets to determine the most promising model for further development. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will use a standard stack of data science tools along with the [LightGBM library](https://lightgbm.readthedocs.io/en/latest/): `pandas`, `numpy`, `matplotlib`, `seaborn`, `sklearn`, `lightgbm`. We also import `gc` for memory management (garbage collection), `warnings` to filer out warnings from `pandas`, and the `preprocess_data` function we wrote in the [Preprocessing notebook](https://bitbucket.org/willkoehrsen/prediction-documentation/src/670f76b3c327aed7916cd2d73f3f84dec8d71c98/notebooks/Preprocessing.ipynb?at=master&fileviewer=file-view-default). Please refer to the `requirements.txt` file for the correct version of the packages to install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Modeling library\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Using KFold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Encoding categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Scikit-Learn Machine Learning models\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "\n",
    "# Memory management\n",
    "import gc\n",
    "\n",
    "# Prepocess data for machine learning\n",
    "from utilities import preprocess_data, implement_model\n",
    "\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Example Data and Preprocess\n",
    "\n",
    "In this notebook we will work with two example datasets to illustrate the process of building and using the model. The preprocessing is done using the function developed in the Preprocessing notebook. As with the other six models demonstrated in the Modeling notebook, the gradient boosting machine will be eventually be run on hundreds of buildings in order to get an accurate assessment of its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>biz_day</th>\n",
       "      <th>week_day_end</th>\n",
       "      <th>ghi</th>\n",
       "      <th>dif</th>\n",
       "      <th>gti</th>\n",
       "      <th>temp</th>\n",
       "      <th>rh</th>\n",
       "      <th>pwat</th>\n",
       "      <th>ws</th>\n",
       "      <th>...</th>\n",
       "      <th>yday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>wday_sin</th>\n",
       "      <th>wday_cos</th>\n",
       "      <th>num_time_sin</th>\n",
       "      <th>num_time_cos</th>\n",
       "      <th>sun_rise_set_neither</th>\n",
       "      <th>sun_rise_set_rise</th>\n",
       "      <th>sun_rise_set_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394397</td>\n",
       "      <td>0.245303</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629749</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390086</td>\n",
       "      <td>0.250522</td>\n",
       "      <td>0.067949</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629749</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.533050</td>\n",
       "      <td>0.998907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>0.256785</td>\n",
       "      <td>0.067949</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629749</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.565955</td>\n",
       "      <td>0.995631</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383621</td>\n",
       "      <td>0.262004</td>\n",
       "      <td>0.067949</td>\n",
       "      <td>0.303279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629749</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.598572</td>\n",
       "      <td>0.990187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.268267</td>\n",
       "      <td>0.067949</td>\n",
       "      <td>0.303279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629749</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.630758</td>\n",
       "      <td>0.982600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  biz_day  week_day_end  ghi  dif  gti      temp        rh  \\\n",
       "0   0.000000      1.0           0.0  0.0  0.0  0.0  0.394397  0.245303   \n",
       "1   0.000011      1.0           0.0  0.0  0.0  0.0  0.390086  0.250522   \n",
       "2   0.000022      1.0           0.0  0.0  0.0  0.0  0.387931  0.256785   \n",
       "3   0.000034      1.0           0.0  0.0  0.0  0.0  0.383621  0.262004   \n",
       "4   0.000045      1.0           0.0  0.0  0.0  0.0  0.379310  0.268267   \n",
       "\n",
       "       pwat        ws        ...         yday_cos  month_sin  month_cos  \\\n",
       "0  0.069231  0.295082        ...         0.629749   0.066987       0.75   \n",
       "1  0.067949  0.295082        ...         0.629749   0.066987       0.75   \n",
       "2  0.067949  0.295082        ...         0.629749   0.066987       0.75   \n",
       "3  0.067949  0.303279        ...         0.629749   0.066987       0.75   \n",
       "4  0.067949  0.303279        ...         0.629749   0.066987       0.75   \n",
       "\n",
       "   wday_sin  wday_cos  num_time_sin  num_time_cos  sun_rise_set_neither  \\\n",
       "0       1.0      0.25      0.500000      1.000000                   1.0   \n",
       "1       1.0      0.25      0.533050      0.998907                   1.0   \n",
       "2       1.0      0.25      0.565955      0.995631                   1.0   \n",
       "3       1.0      0.25      0.598572      0.990187                   1.0   \n",
       "4       1.0      0.25      0.630758      0.982600                   1.0   \n",
       "\n",
       "   sun_rise_set_rise  sun_rise_set_set  \n",
       "0                0.0               0.0  \n",
       "1                0.0               0.0  \n",
       "2                0.0               0.0  \n",
       "3                0.0               0.0  \n",
       "4                0.0               0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in example data\n",
    "df = pd.read_csv('../data/f-APS_weather.csv')\n",
    "\n",
    "# Preprocess for machine learning\n",
    "train, targets, test, test_targets = preprocess_data(df)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready for machine learning: all of the features are numeric, there are no missing values, and the features have been scaled between 0 and 1. The [gradient boosting machine can handle missing values](https://github.com/Microsoft/LightGBM/issues/122) and does not need scaled features, but because we are comparing performance to other models, we will use the same standard set of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metric\n",
    "\n",
    "The evaluation metric we selected is the Mean Absolute Percentage Error (MAPE). The definition of MAPE is the average of the absolute residuals divided by the true values: \n",
    "\n",
    "$$\\mbox{MAPE} = 100\\% * \\frac{1}{n}\\sum_{i=1}^n  \\left|\\frac{y_i-\\hat{y}_i}{y_i}\\right|$$\n",
    "\n",
    "In order to use this evaluation metric in LightGBM, we need to write a [custom evaluation metric function](https://github.com/Microsoft/LightGBM/issues/284) that takes in the true targets and the predictions. The output of this function must be three arguments: \n",
    "\n",
    "* A `string` representing the name of the metric\n",
    "* A `float` for the value of the metric\n",
    "* A `boolean` stating if a higher value is better (which is False because a lower MAPE is better)\n",
    "\n",
    "We can pass in this function to the model during training to be used as the evaluation metric for early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "def mape(true, predictions):\n",
    "    \"\"\"Calculates the mean absolute percentage error given the true\n",
    "    values and predictions. Return is formatted for a LightGBM custom evaluation metric.\"\"\"\n",
    "    \n",
    "    return 'mape', 100 * np.mean(abs(true - predictions) / true), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Regression Model\n",
    "\n",
    "Again these sources are extremely helpful ([Tree Boosting with XGBoost](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf?sequence=1&isAllowed=y), [Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf), and [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do))\n",
    "\n",
    "A gradient boosting machine has many hyperparameters to tune. In this notebook, and for evaluation of all buildings, we will use a standard set of hyperparameters (for all but one of the settings) selected by analyzing best practices recommended by research papers and the documentation for the library. Moreover, we will use early stopping to determine one of the most important hyperparameters, the number of decision trees trained (this is also known as the number of boosting rounds or the number of iterations). \n",
    "\n",
    "## Gradient Boosting Machine Hyperparameters \n",
    "\n",
    "There are two sets of hyperparameters in a gradient boosting machine: those that apply to the overall ensemble, and those that apply to the individual learners in the ensemble. These hyperparameters are used to control the bias/variance tradeoff: higher variance leads to overfitting while high bias leads to underfitting. Both situations decrease generalization performance on the test set.\n",
    "\n",
    "\n",
    "### Ensemble Hyperparameters\n",
    "\n",
    "The two main hyperparameters that pertain to the entire ensemble are:\n",
    "\n",
    "* `n_estimators`: number of base learners used, equivalently the number of boosting rounds. More estimators decreases the bias but increases the variance which can lead to overfitting. This hyperparameter can be set using early stopping. \n",
    "* `learning_rate`: the contribution of each new learner to the ensemble. A larger learning rate will increase the rate of convergence but may lead the algorithm to jump around the optimum (lowest point) of the objective function. A lower learning rate will lead to longer training times but can improve generalization on the test set. \n",
    "\n",
    "There are a few other hyperparameters that deal with the entire model. However, we will only set one other different than the default:\n",
    "\n",
    "* `subsample`: the fraction of observations to use for training each base learner. By default the GBM uses all of the training examples, but by randomly samping a subset of the observations, the variance of the model can be reduced. \n",
    "\n",
    "We will use the following set of ensemble hyperparameters (all others are kept at the defaults):\n",
    "\n",
    "* `n_estimators=10000`: this number of base learners will likely not be reached because of early stopping\n",
    "* `learning_rate=0.01`: This is a decrease of three-quarters from the default of 1.0\n",
    "* `subsample=0.9`: sample 90% of the training examples for training\n",
    "\n",
    "The number of estimators will not actually be 10,000 for many of the training runs because we will use early stopping to determine the ideal value. \n",
    "\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Early stopping continues adding learners to the ensemble until the error on a validation set has not decreased for a set number of iterations. For example, here we will use early stopping by adding more learners until the MAPE on the validation set has not decreased for 100 iterations. The model records the number of estimators that resulted in the lowest error and then this number of estimators is used to make predictions on the test set.\n",
    "\n",
    "The concept of early stopping is illustrated in the following image:\n",
    "\n",
    "![image](../images/early_stopping.png)\n",
    "\n",
    "\n",
    "Early stopping is an effective technique for choosing the ideal number of base learners and is commonly used for training gradient boosting machines. Early stopping greatly simplifies the process of finding the optimal number of iterations. Using early stopping does require a validation set though, which reduces the amount of training data. However, we can get around this limitation by selecting the number of iterations using the technique of cross validation.\n",
    "\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "Cross validation (cv) is another best practice in machine learning. Often, the ideal model hyperparameters require a validation set to determine. The validation set must be drawn from the __training data__ and not the testing data. Splitting the training data into two sets though reduces the amount of training data, which can have a detrimental effect on performance. Moreover, optimizing the hyperparameters for a single validation set can just lead to overfitting on the _validation data_. Therefore, a technique known as cross-validation is used to avoid the need for a separate validation set and to better predict the generalization error of the model. The most common implementation of cross validation is K-Fold Cross Validation. \n",
    "\n",
    "#### K-Fold Cross Validation\n",
    "\n",
    "In K-Fold cross validation, the training data is split into K folds and then training proceeds in an iterative manner. On each iteration, the model is trained on K-1 of the sets and tested on the Kth set. This continues for K iterations until eventually all of the training data has been used as a validation set. The final estimate of the performance of the model is the average performance across the K validation scores. This method eliminates the need to split the valuable training data and usually provides a better measure of the generalization performance than using one validation set. The final model is then trained on the entire training data before making predictions. \n",
    "\n",
    "Below is an image of using 5-fold cv:\n",
    "\n",
    "![image](../images/5-fold.png)\n",
    "\n",
    "[Source](https://tex.stackexchange.com/questions/429451/k-fold-cross-validation-figure-using-tikz-or-table)\n",
    "\n",
    "We will use K-Fold cross validation in order to determine the ideal number of base learners. We will split the data into 5 folds, and each fold, train until the MAPE on the validation fold does not decrease for 100 iterations. Each iteration, the best number of learners is recorded, and the final number of learners used is the average of the best number from the 5 validation runs. After completing 5-fold cv, the model is trained on the entire set of training data using the optimal number of iterations. Ideally this procedure will find a near-optimal number of iterations that will result in high generalization performance on the test set. \n",
    "\n",
    "While K-Fold cv can be used to select the number of estimators, the other hyperparameters will have to be set at a constant value for all of the evaluation. The other main hyperparameter governing the entire model is the learning rate which will be set at 0.01. Generally, a smaller learning rate is used to complement a larger number of estimators. The maximum number of estimators is 10000, and although this number likely will not be reached because of early stopping, the learning rate is set at 0.01 to prevent the model from \"jumping\" around the optimum value of the objective function. \n",
    "\n",
    "\n",
    "## Regularization and Individual Learner Hyperparameters\n",
    "\n",
    "The Gradient Boosting Machine can be regularized both on the ensemble level and the base learner level. To regularize the model on the ensemble level, the number of estimators can be reduced. On the individual learner level, there are a number of hyperparameters that can be adjusted. We can limit the complexity of each tree by limiting the maximum depth, establishing a minimum number of observations required in each leaf node, or limiting the maximum number of leaf nodes among other methods. We can also penalize the complexity of the trees by adding terms to the objective function proportional to the L1 and L2 norm of the weights of the tree. We will impose regularization on the individual learners by setting the following hyperparameters:\n",
    "\n",
    "* `reg_alpha=0.1`: Penalty on L1 norm of the tree weights\n",
    "* `reg_lambda=0.1`: Penality on L2 norm of the tree weights\n",
    "\n",
    "The final hyperparameters we set in the call to the model are:\n",
    "\n",
    "* `n_jobs=-1`: Use all available cores on the machine for training\n",
    "\n",
    "__All of the rest of the hyperparamters are set at the defaults.__ These defaults should be reviewed and can be [ound in the documentation](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Set Up\n",
    "\n",
    "Here we build the model with the specified hyperparameters. In this run I also set the `random_state` to ensure consistent results across runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.01, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=10000,\n",
       "       n_jobs=-1, num_leaves=31, objective=None, random_state=100,\n",
       "       reg_alpha=0.1, reg_lambda=0.1, silent=True, subsample=0.9,\n",
       "       subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model with specified hyperparameters\n",
    "model = lgb.LGBMRegressor(n_estimators=10000,\n",
    "                          learning_rate = 0.01, \n",
    "                          reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                          subsample = 0.9, n_jobs = -1,\n",
    "                          random_state=100)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Applied\n",
    "\n",
    "We will use five-fold cross validation for early stopping. To actually split the data, we have to convert the features into numpy arrays. \n",
    "\n",
    "In the code below, we make the [`KFold` object](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html), and then show a single iteration of splitting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape for Fold:  (71416, 21)\n",
      "Validation Data Shape for Fold:  (17855, 21)\n"
     ]
    }
   ],
   "source": [
    "# Kfold cross validation\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Split the training data\n",
    "for i, (train_indices, valid_indices) in enumerate(kfold.split(np.array(train))):\n",
    "    \n",
    "    if i > 0: \n",
    "        break\n",
    "    \n",
    "    # Training data for fold\n",
    "    train_features, train_targets = np.array(train)[train_indices], targets[train_indices]\n",
    "    \n",
    "    # Validation data for fold\n",
    "    valid_features, valid_targets = np.array(train)[valid_indices], targets[valid_indices]\n",
    "\n",
    "    print('Training Data Shape for Fold: ', train_features.shape)\n",
    "    print('Validation Data Shape for Fold: ', valid_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each fold will be used for validation exactly once. The validation data is used to determine the number of estimators (rounds of boosting) used by the model on each fold. The number of estimators used in the final full round of training will be the average of the best number of estimators returned from each round of cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "To illustrate what will happen on each fold, we can train the model on a single fold using the validation data for early stopping. We pass the model:\n",
    "\n",
    "* `X = train_features`: training features\n",
    "* `y = train_targets`: training targets\n",
    "* `early_stopping_rounds = 100`: Stop training after the validation metric has not decreased for 100 rounds\n",
    "* `eval_metric = mape`: evaluation metric used for early stopping. The model will stop training when this metric does not decrease for the specified number of early stopping rounds.\n",
    "* `eval_set = [(valid_features, valid_labels),(train_features, train_labels)]`: evaluation data to be used for early stopping. The model will only use the validation data for early stopping but will still evaluate the metrics for the training data. We can use the difference between the metrics to assess the amount of overfitting.\n",
    "* `eval_names = ['valid', 'train']`: names of the evaluation sets of data\n",
    "* `verbose = 500`: print out statistics about training and validation scores every 500 rounds (every 500 estimators trained)\n",
    "\n",
    "Let's see what this looks like in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.634723\tvalid's mape: 9.06663\ttrain's l2: 0.682509\ttrain's mape: 9.29991\n",
      "[1000]\tvalid's l2: 0.594519\tvalid's mape: 8.80087\ttrain's l2: 0.544038\ttrain's mape: 8.05561\n",
      "Early stopping, best iteration is:\n",
      "[1025]\tvalid's l2: 0.592757\tvalid's mape: 8.79411\ttrain's l2: 0.539732\ttrain's mape: 8.02122\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X = train_features, y = train_targets, \n",
    "          early_stopping_rounds = 100,\n",
    "          eval_metric = mape,\n",
    "          eval_set = [(valid_features, valid_targets), \n",
    "                      (train_features, train_targets)],\n",
    "          eval_names = ['valid', 'train'], verbose = 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping kicked in at round 1025 (it might be different depending on the run) because the error on the validation score had not improved for 100 rounds. We can use the best number of iterations found on this validation fold to make predictions on the test set. First, we rebuild the model and fit on the entire training set using the optimum number of iterations, and then we make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of iterations:  1025\n"
     ]
    }
   ],
   "source": [
    "print('Best number of iterations: ', model.best_iteration_)\n",
    "\n",
    "\n",
    "# Create the model with specified hyperparameters\n",
    "model = lgb.LGBMRegressor(n_estimators=model.best_iteration_,\n",
    "                          learning_rate = 0.01, \n",
    "                          reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                          subsample = 0.9, n_jobs = -1,\n",
    "                          random_state=100)\n",
    "\n",
    "\n",
    "model.fit(train, targets)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(np.array(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can score the model using the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on the test set: 16.23532\n"
     ]
    }
   ],
   "source": [
    "_, mape_score, _ = mape(test_targets, predictions)\n",
    "print('MAPE on the test set: %0.5f' % mape_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used a differerent validation fold then the ideal number of iterations would likely have been different. This is the reason we used 5-fold cv: we want to get a better idea of the best number of iterations than a single validation would show. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "To actually use cross validation, we need to repeat the same process 4 more times, using the other 4 folds as validation once each. To use this process, we can write a simple loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.634723\tvalid's mape: 9.06663\ttrain's l2: 0.682509\ttrain's mape: 9.29991\n",
      "[1000]\tvalid's l2: 0.594519\tvalid's mape: 8.80087\ttrain's l2: 0.544038\ttrain's mape: 8.05561\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1025]\tvalid's l2: 0.592757\tvalid's mape: 8.79411\ttrain's l2: 0.539732\ttrain's mape: 8.02122\n",
      "\n",
      "Fold 0 \t Validation MAPE: 8.79411\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 2.01266\tvalid's mape: 13.2204\ttrain's l2: 0.529106\ttrain's mape: 8.40169\n",
      "Early stopping, best iteration is:\n",
      "[629]\tvalid's l2: 1.99781\tvalid's mape: 13.1469\ttrain's l2: 0.483788\ttrain's mape: 7.92668\n",
      "\n",
      "Fold 1 \t Validation MAPE: 13.14687\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.83468\tvalid's mape: 11.3004\ttrain's l2: 0.651426\ttrain's mape: 8.58868\n",
      "[1000]\tvalid's l2: 0.759987\tvalid's mape: 10.4665\ttrain's l2: 0.518724\ttrain's mape: 7.46587\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1021]\tvalid's l2: 0.759038\tvalid's mape: 10.4607\ttrain's l2: 0.515648\ttrain's mape: 7.44514\n",
      "\n",
      "Fold 2 \t Validation MAPE: 10.46074\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 2.18618\tvalid's mape: 12.6163\ttrain's l2: 0.532689\ttrain's mape: 8.34055\n",
      "Early stopping, best iteration is:\n",
      "[628]\tvalid's l2: 2.13564\tvalid's mape: 12.5569\ttrain's l2: 0.486273\ttrain's mape: 7.88759\n",
      "\n",
      "Fold 3 \t Validation MAPE: 12.55694\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 1.20289\tvalid's mape: 17.6847\ttrain's l2: 0.640655\ttrain's mape: 8.51282\n",
      "[1000]\tvalid's l2: 1.09902\tvalid's mape: 15.8451\ttrain's l2: 0.501854\ttrain's mape: 7.35708\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[974]\tvalid's l2: 1.09834\tvalid's mape: 15.8501\ttrain's l2: 0.506062\ttrain's mape: 7.39222\n",
      "\n",
      "Fold 4 \t Validation MAPE: 15.85013\n",
      "\n",
      "\n",
      " Best Number of Iterations:  855\n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "\n",
    "# Split the training data\n",
    "for i, (train_indices, valid_indices) in enumerate(kfold.split(np.array(train))):\n",
    "\n",
    "    # Training data for fold\n",
    "    train_features, train_targets = np.array(train)[train_indices], targets[train_indices]\n",
    "    \n",
    "    # Validation data for fold\n",
    "    valid_features, valid_targets = np.array(train)[valid_indices], targets[valid_indices]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X = train_features, y = train_targets, \n",
    "              early_stopping_rounds = 100,\n",
    "              eval_metric = mape,\n",
    "              eval_set = [(valid_features, valid_targets), \n",
    "                          (train_features, train_targets)],\n",
    "              eval_names = ['valid', 'train'], verbose = 500)\n",
    "    \n",
    "    # Add the number of iterations to the total for averaging\n",
    "    iterations += model.best_iteration_\n",
    "    \n",
    "    # Evaluate the mape\n",
    "    _, valid_mape, _ = mape(valid_targets, model.predict(valid_features, \n",
    "                                                         num_iteration=model.best_iteration_))\n",
    "    \n",
    "    print('\\nFold %d \\t Validation MAPE: %0.5f\\n' % (i, valid_mape))\n",
    "    \n",
    "iterations = int(iterations / kfold.n_splits)\n",
    "print('\\n Best Number of Iterations: ', iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, we can see the model overfits because the validation error is significantly higher than the training error. This can be addressed by regularizing the overall model or the individual learners in the ensemble. In this case, since we are already using Early Stopping, we probably would want to consider regularization on an individual learner basis. For now, we will not add any more regularization. If the model performs well on all the buildings, then we can come back and address the overfitting issue. This would involve using random search with cross validation to find the regularization hyperparameters that perform the best. \n",
    "\n",
    "Once the cross validation has finished, we need to retrain the model on the entire training set. This time, we will use the average number of optimum iterations that were returned from the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on the test set: 16.44667\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model with the optimal number of estimators\n",
    "model = lgb.LGBMRegressor(n_estimators=iterations,\n",
    "                          learning_rate = 0.01, \n",
    "                          reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                          subsample = 0.9, n_jobs = -1)\n",
    "\n",
    "# Refit on the entire training data\n",
    "model.fit(train, targets)\n",
    "\n",
    "\n",
    "_, mape_score, _ = mape(test_targets, model.predict(np.array(test), num_iteration=iterations))\n",
    "print('MAPE on the test set: %0.5f' % mape_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, the best model on this dataset on the six Scikit-Learn models was the random forest with a test MAPE of 15.70. We will have to use more data in order to find out which model really is best! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "Machine learning is often criticized as a black box: we put in some data and recieve answers - often extremely accurate answers - with no explanations. One method we can use to peer into the black box of gradient boosting machines are feature importances. For a gradient boosting model based on individual decision trees, the [feature importances represent the decrease in impurity from including the feature in the model](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined). The absolute value of the feature importances can be [difficult to interpret](https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf), but the relative magnitude of the importances can be used to determine which features the model considers \"most important\". \n",
    "We can use these importances for feature selection (which is more important when the number of features is large) or to try and understand how the model makes predictions.\n",
    "\n",
    "Feature importances can be extracted from a trained gradient boosting model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_time_sin</td>\n",
       "      <td>3869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>num_time_cos</td>\n",
       "      <td>3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>temp</td>\n",
       "      <td>2503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yday_sin</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yday_cos</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dif</td>\n",
       "      <td>1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biz_day</td>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghi</td>\n",
       "      <td>1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pwat</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wday_sin</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rh</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wday_cos</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ws</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gti</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sun_rise_set_neither</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>month_cos</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month_sin</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>week_day_end</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sun_rise_set_set</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sun_rise_set_rise</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "0              timestamp        4577\n",
       "16          num_time_sin        3869\n",
       "17          num_time_cos        3013\n",
       "6                   temp        2503\n",
       "10              yday_sin        1874\n",
       "11              yday_cos        1782\n",
       "4                    dif        1645\n",
       "1                biz_day        1197\n",
       "3                    ghi        1018\n",
       "8                   pwat         980\n",
       "14              wday_sin         866\n",
       "7                     rh         615\n",
       "15              wday_cos         377\n",
       "9                     ws         366\n",
       "5                    gti         320\n",
       "18  sun_rise_set_neither         283\n",
       "13             month_cos         202\n",
       "12             month_sin         129\n",
       "2           week_day_end          16\n",
       "20      sun_rise_set_set          10\n",
       "19     sun_rise_set_rise           8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(train.columns)\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Dataframe of feature importances\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, \n",
    "                                    'importance': importances})\n",
    "\n",
    "# Sort the features by their importance\n",
    "feature_importances.sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature is the `timestamp` which we converted to a numeric to represent the number of seconds since the beginning of the data. The second most important feature is `num_time_sin`, one of the cyclical representations of the time of day, followed by the `temp` (the temperature at the building's location). These importances makes sense based on our domain knowledge because energy use is highly dependent on the time of day and the temperature. While it may be a mistake to read too much into the feature importances, it is reassuring that they agree with our domain knowledge. \n",
    "\n",
    "We can make a quick plot to show the normalized feature importances where each importance is divided by the sum of the importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance from a model (usually a tree-based model). \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df : dataframe\n",
    "            feature importances. Must have the features in a column\n",
    "            called `features` and the importances in a column called `importance\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        shows a plot of the 15 most importance features\n",
    "        \n",
    "        df : dataframe\n",
    "            feature importances sorted by importance (highest to lowest) \n",
    "            with a column for normalized importance\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAGECAYAAADA7m3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xm8XWV97/HPF0Q4CRgkDldqbTANeilokMEB8OLQasUBKw6VqkGl2taKtYBaNKYqVatVkzq1oYKKVRxAEVTQAoIRlSlMih4EvApeZyOYAzL87h97BTeHs0+mlbOH83m/Xvt11n7Ws571W7/sbH55eNY6qSokSZIk3d1W/Q5AkiRJGlQWy5IkSVIPFsuSJElSDxbLkiRJUg8Wy5IkSVIPFsuSJElSDxbLkiRJUg8Wy5JmvSQnJKkpXs9r+Ty3JVnS5pibGMc5SY7rdxzTSbJ/82ewoN+xSJrd7tHvACRpQJwHPGdS26/7EciGSHLPqvpdv+PYEpLcs98xSNI6zixLUsfvqur/TXrdvG5nkuclWZ3k5iTXJXlXkrld+/+0mbH9ZZI1Sb6aZN+u/dcBWwPHr5u5btqXJLmtO5AkD2z6HNi8P7B5f1CSryW5GfjrZt9eSc5MclOSnyU5OckfbcyFN3H/V5K3JPlpkl8nOTbJVkmWJvlJM/axk467rul3XJLfJPl5krcn2aqrzw5J/qM5/uYkFyb5s679C5prOzTJF5L8FvhvOv94Abi22X9O0/8RSb7YxHlTkguSPHmKuN6UZHnz5/GTJO9MsvWkfn+X5NtJbmnG+3TXvnskWZbk2ibuK5O8bNLxL03ynWb/L5Kcm+SBG5N7SYPPYlmS1qNZOvEB4N+A3YAXAk8EPtjVbXvgfcCjgMcA48CXksxv9u8D3A68CnhA89pY/wb8K/C/gc8m2Q34KnA+sDfw+OYcX06y3UaOfQiwDbA/8Grgn4DTmus6ADgS+Kckfz7puL8HbqBzff8AvILONa7zIeBJwF8BewKrgNOSPHTSOG+nUyTvAbwGeEbTvi+dXP1F8/5ewCeAA4FHAGcApybZdYq4fgw8EnhlE9ML1+1M8s/NOd/fnPPJwOqu449rzvkyOvl+E/D2JC9pjt+Lzp//W4GHNPF8BEmjp6p8+fLla1a/gBOA24Cbul7f79p/HfDyScc8Fijg3j3G3Ar4FXBoV9ttwJJJ/ZYAt01qe2Az9oHN+wOb9y+YIu5PTGrbFlgLHDzN9Z4DHDfp/epJfa4ELp/Udinwzkl5OW9Sn38BftRs/3ET91Mm9bkY+FCzvaDp84ZJffZv2hdswJ/fpcAxk+I6dVKfLwEfb7bnAhPAkT3G2wW4A3jopPal6/IEPBNYA9yr359fX758bdmXa5YlqeObwIu63t8GkOS+wB8B70ryzq79aX7+MXBBkl3ozD4+GrgfnWJ5TnNsW7416f0+wB8nuWlS+3bAoo0c+9JJ7/9f85rcdr9JbedPer8KeF2Se9GZhQc4d1Kfc+nkqdvka5tS8+fxz3Rm0f8XnXtvtuPueV496f31dIpggD9pjjmzx2n2pvPne2GS7vZ70Jm5B/gycA2dZSJfBs4CTq6qn2/IdUgaHhbLktQxUVVXT9G+brnaEcDZU+z/UfPzNODnwN8BPwR+B3wNWN/NandM0bZNj76/nSK2jwJvm6LvL9Zz3slunfS+erStb/le1rN/XZ+a1Db52no5AXgQcDRwLZ0Z4k9w9zxPvvlxqtgnx7DOun6PoTNLf7djquqmJHsD+9FZkvNy4F+TPKGqLtqgK5E0FCyWJWkaVfWTJD8EHlJVK6fq06xL3o3OcoMzmrYHcvdZ2N/Rucmv20+BrZPcv6p+0rQ9YgPDuxB4GJ0lI70Kvy3tUZPePxq4oap+k+TKpu2xwBe6+hwAXLKecdcVu5Pz9Vjg6Ko6FaC5yfLBwBUbEfO3gZvprKW+fIr964rdB1XVab0Gqarb6cySn5vkjc24z+86XtII8AY/SVq/Y4BXJnl9kt2TPCTJwUn+o9n/K+BnwOFJdk3yaODjdGY9u10LPC7Jzknu07R9C7gReFuSRc2THZZuYFz/QufmsxOT7JtklySPa54C8eDNuN6Nsbh5asSuSZ5PZwb+3QBV9X3gU8D7kzwpyUOTLAd2B96xnnF/QGfW/SlJ7pdkXtP+XeDQJHskWUwnz5ML6mlV1U10bpZc1jwRY9ckD0/yumb/1XRuTFyZ5AVJ/rjZ/+IkrwFI8owk/5DO00geBBwM/CGdglnSCLFYlqT1qKqP0nkG80F0itsLgGV01sFSVXcAzwYWApfRWSrwHjpPY+j2j8BedIrmnzXH/hL4SzoztJcBb6CzxGBD4voOnaUC29N5KsS3gZXAGDP3jOh/p7Ne+ELgvXSeGvLurv0vbWI7kc666P2Ap1bVVdMN2syyvw54LZ08fq7ZdRid/3Z9C/gsnRv3LtiEuN9A848gOrPSZ3LXGf2/bq7jGDp5/R86a9qvafb/Cnhac/7v0XlKyVvoFNmSRkj693/uJEnDLJ1nRx9XVW/pdyyStKU4syxJkiT1YLEsSZIk9eAyDEmSJKkHZ5YlSZKkHnzOcgvWrFnj9LwkSdIImDdv3l1+uZIzy5IkSVIPFsuSJElSDxbLGkjj4+P9DmFkmMt2mc92mc/2mMt2mc/2DHsuLZYlSZKkHiyWJUmSpB4sliVJkqQeLJYlSZKkHiyWJUmSpB4sliVJkqQeLJYlSZKkHiyWJUmSpB4sliVJkqQeLJYlSZKkHiyWJUmSpB4sliVJkqQe7tHvAEbNQUcs63cII2Fi7QRjc8b6HcZIMJftMp/tMp/tMZftMp/t2dhcLpw/lxVLj9qCEW0ci+WWrVp8eL9DkCRJGl6rV/Y7grtwGYYkSZLUg8WyJEmS1MOMFMtJdkzyt832zkk+vQXPtTjJU7bU+JIkSZo9ZmpmeUfgbwGq6oaqOmQLnmsxYLEsSZKkzTZTN/i9DViYZDUwDvzvqto9yRLgYGBrYHfg34B7Ai8AbgGeUlW/TLIQeB9wX2AtcHhVXZXk2cAbgduBNcATgTcBY0n2B94KXAu8BxgDJoDDquq7G3Huc4DVwL7AvYAXV9W3tlSiJEmSNDhmamb5tcD3q2oxMPlZILsDz6dTjB4LrK2qPYHzgRc2ff4T+Puq2gs4Enh/074UeFJVPRx4elX9rmk7qaoWV9VJwFXAY5sxlwL/spHnBphbVY+hMzv+oc1LhSRJkobFIDw67uyquhG4Mcka4PNN++XAw5JsDzwG+FSSdcds2/xcBZyQ5JPAyT3Gnwd8OMkioIBtNvTcXf0+DlBV5ya5V5Idq+rXm3i9kiRJ6mFi7QTj4+Mzes5Fixb13DcIxfItXdt3dL2/g058WwG/bmal76KqXp7kkcBBwOokd+sDvJlOUfzMJAuAczbi3HeeavKpp7keSZIkbaKxOWPTFq8zbaaWYdwI7LApB1bVb4Brm/XJpOPhzfbCqvpmVS0Ffg784RTnmgdc32wv2bTweW5zvv2BNVW1ZhPHkSRJ0hCZkWK5qn4BrEpyBfCOTRjiUOAlSS4FrgSe0bS/I8nlzbjnApcCZwO7JVmd5LnAvwJvTbKKzs18m+JXSb4OfBB4ySaOIUmSpCGTKlcUTKd5GsaRVXVhrz5r1qy5M4k7Hn99r26SJElaj/1Wr+T05cv6dv558+al+72/wU+SJEnqYRBu8BtoVXVgv2OQJElSfzizLEmSJPXgzHLL9lu9st8hjISJtROMzRnrdxgjwVy2y3y2y3y2x1y2y3y2Z2NzuXD+3C0YzcbzBr8WdN/gp3aMj48P1DMWh5m5bJf5bJf5bI+5bJf5bM+w5dIb/CRJkqQNZLEsSZIk9eCa5ZYddMSyfocwElwr1h5z2S7z2S7z2R5z2a7ufC6cP5cVS4/qc0TqF4vllq1afHi/Q5AkSW3y5v1ZzWUYkiRJUg8Wy5IkSVIPFsuSJElSDwNZLCdZkmTnrvfHJdlthmP4QpIdZ/KckiRJGiyDeoPfEuAK4AaAqnrpTAdQVU+Z6XNKkiRpsGzwzHKSBUm+k2RlkiuTnJlkLMk5SfZu+twnyXXN9pIkn03y+STXJnlFklcnuSTJN5Ls1OM8hwB7Ax9LsnqKc9yU5O1JLkrylST7NvuvSfL0ps/WSd6R5IIklyV52TTX9YAk5zbnuiLJAU37dc31THndG5o3SZIkDa+NnVleBPxlVR2e5JPAs9bTf3dgT2A74GrgNVW1Z5J3Ay8E3jP5gKr6dJJXAEdW1YUAyV1+6+Bc4Jyqek2SU4C3AH8K7AZ8GDgVeAmwpqr2SbItsCrJmVV17RQxPh84o6qOTbI1MGcDr/vE9Vy7JEkaARNrJxgfH+93GENt0PM33a/j3thi+dqqWt1sXwQsWE//s6vqRuDGJGuAzzftlwMP28hzr/M74Etd49xSVbcmubwrnj8DHtbMUgPMo1PwTlUsXwB8KMk2wGe7rq/bxl63JEkaEWNzxqYtpjS98fHxoc7fxt7gd0vX9u10iu3busbZbpr+d3S9v4NNXy99a1XV5DGrqnvMAH9fVYub1y5VdeZUg1XVucBjgeuBjyZ54RTdprpuSZIkjbg2noZxHbBXs33INP02xo3ADptx/BnA3zSzxSTZNcncqTom+SPgp1W1Evgv4BGbcV5JkiSNkDZmSN8JfDLJC4CzWhgP4ATgg0kmgEdvwvHH0VkqcXE6C55/Bhzco++BwFFJbgVuorOWWpIkSSK/X9GgTbVmzZo7k7jj8df3MxRJktSy/Vav5PTly/odxtAatjXL8+bNu8uTJQbyl5JIkiRJg6CvN6oleR+w36Tm5VV1/BY41x7ARyc131JVj2z7XJIkSRoNfS2Wq+rvZvBclwOLt/R59lu9ckufYlaYWDvB2Bx/90sbzGW7zGe7zGd7zGW7uvO5cP6UzwjQLOEj0FrmmqZ2DNv6pkFmLttlPttlPttjLttlPrWOa5YlSZKkHiyWJUmSpB5chtGyg45Y1u8QRoJr79pjLttlPts1W/K5cP5cViw9qt9hSNoEFsstW7X48H6HIEkaNN78LQ0tl2FIkiRJPVgsS5IkST1YLEuSJEk9DFSxnGRJkp273h+XZLd+xiRJkqTZa9Bu8FsCXAHcAFBVL+1rNJIkSZrV1juznGRBku8kWZnkyiRnJhlLck6SvZs+90lyXbO9JMlnk3w+ybVJXpHk1UkuSfKNJDv1OM8hwN7Ax5KsnuIcNyV5e5KLknwlyb7N/muSPL3ps3WSdyS5IMllSV62nms7OsnlSS5N8rambXET52VJTkly76b9lUm+3bR/YoMzLEmSpKG1oTPLi4C/rKrDk3wSeNZ6+u8O7AlsB1wNvKaq9kzybuCFwHsmH1BVn07yCuDIqroQIEl3l7nAOVX1miSnAG8B/hTYDfgwcCrwEmBNVe2TZFtgVZIzq+rayedL8ufAwcAjq2ptVxH/EeDvq+qrSd4EvBF4FfBaYJequiXJjuu5fkmS7jSxdoLx8fEtfp6ZOMdsYj7bM+i5nO5Xm29osXxtVa1uti8CFqyn/9lVdSNwY5I1wOeb9suBh23gOSf7HfClrnFuqapbk1zeFc+fAQ9rZqkB5tEp9O9WLANPBI6vqrUAVfXLJPOAHavqq02fDwOfarYvozPr/Vngs5t4DZKkWWhszti0/zFuw/j4+BY/x2xiPtsz7Lnc0GL5lq7t24Ex4DZ+v4xju2n639H1/o6NOOdkt1ZVTR6zqu5Ism7M0JkVPmMDxgtQ6+31ewcBjwWeDrwhyZ9U1W0bcbwkSZKGzOY8DeM6YK9m+5Bp+m2MG4EdNuP4M4C/SbINQJJdk8zt0fdM4MVJ5jR9d6qqNcCvkhzQ9HkB8NUkWwF/WFVnA0cDOwLbb0ackiRJGgKb8zSMdwKfTPIC4KyW4jkB+GCSCeDRm3D8cXSWZFyczoLnn9FZl3w3VfWlJIuBC5P8DvgC8E/Ai5oY5gDXAIcBWwMnNss0Ary7qn69CfFJkiRpiOT3Kxu0qdasWXNnEnc8/vp+hiJJGkD7rV7J6cuXbdFzDPu60EFjPtszbLmcN2/eXZ4wMVC/lESSJEkaJH35pSRJ3gfsN6l5eVUdvwXOtQfw0UnNt1TVI9s+lyRJkkZLX4rlqvq7GTzX5cDimTrffqtXztSpRtrE2gnG5oz1O4yRYC7bZT7bNVvyuXB+r3vNJQ26Qft110NvS69Jmy2GbX3TIDOX7TKf7TKfkgada5YlSZKkHiyWJUmSpB5chtGyg45Y1u8QRsJsWcc4E8xlu8znxlk4fy4rlh7V7zAkaZNZLLds1eLD+x2CJA0Ob3qWNORchiFJkiT1YLEsSZIk9TCSxXKSHZP8bb/jkCRJ0nAbyWIZ2BGwWJYkSdJmGdVi+W3AwiSrk7wjyVFJLkhyWZJ/BkiyIMlVSY5LckWSjyV5YpJVScaT7Nv0W5bko0nOatq9g0+SJGmWGNVi+bXA96tqMfBlYBGwL51fe71Xksc2/f4YWA48DHgo8Hxgf+BI4J+6xnsYcBDwaGBpkp1n4iIkSZLUX7Ph0XF/1rwuad5vT6d4/r/AtVV1OUCSK4H/qapKcjmwoGuMz1XVBDCR5Gw6hfdnZyh+SRpaE2snGB8fn7bP+vZrw5nLdpnP9gx6LhctWtRz32wolgO8tar+4y6NyQLglq6mO7re38Fdc1OTxpz8XpI0hbE5Y9P+R2h8fHza/dpw5rJd5rM9w57LUV2GcSOwQ7N9BvDiJNsDJPmDJPfbyPGekWS7JPOBA4ELWotUkiRJA2skZ5ar6hfNjXpXAF8E/hs4PwnATcBfAbdvxJDfAk4HHgS8uapuaDlkSZIkDaCRLJYBqur5k5qWT9Ft967+S7q2r+veB3yvqv66zfgkSZI0+EZ1GYYkSZK02UZ2ZrktVbWs3zFIkiSpP5xZliRJknpwZrll+61e2e8QRsLE2gnG5oz1O4yRYC7bZT43zsL5c/sdgiRtFovllp2+fFm/QxgJw/5MxkFiLttlPiVpdnEZhiRJktSDxbIkSZLUg8swWnbQEcv6HcJIcF1oe8zlplk4fy4rlh7V7zAkSX1msdyyVYsP73cIktrgzbqSJFyGIUmSJPVksSxJkiT1YLEsSZIk9TB0xXKSE5IcsoXP8fIkL9yS55AkSdLg8wa/KVTVB/sdgyRJkvpvoGaWk7w5yRFd749NckSS9yb5dpLTgft17V+a5IIkVyT5z3QsTHJxV59FSS6a5pxva8a+LMk7m7ZlSY5sts9J8vYk30ryvSQHbJGLlyRJ0sAZtJnl/wJOBpYn2Qp4HnA08FRgD+D+wLeBDzX931tVbwJI8lHgqVX1+SRrkiyuqtXAYcAJU50syU7AM4GHVlUl2bFHXPeoqn2TPAV4I/DEFq5V0gCbWDvB+Pj4lPt6tWvTmM/2mMt2mc/2DHouFy1a1HPfQBXLVXVdkl8k2ZNOYXwJcADw8aq6HbghyVldhzwuydHAHGAn4Erg88BxwGFJXg08F9i3xyl/A9wMHNfMWp/Wo9/Jzc+LgAWben2ShsfYnLEpvzzHx8en/VLVxjGf7TGX7TKf7Rn2XA7UMozGccASOjPC62aQa3KnJNsB7wcOqao9gJXAds3uzwB/TmdG+qKq+sVUJ6qq2+gU0p8BDga+1COmW5qftzNg/8CQJEnSljOIxfIpwJOBfYAzgHOB5yXZOskDgMc1/dYVxj9Psj1w5xMyqurm5tgPAMf3OlFz3Lyq+gLwKmBxy9ciSZKkITZws6RV9bskZwO/rqrbk5wCPB64HPge8NWm36+TrGzarwMumDTUx4C/AM6c5nQ7AJ9rZqkD/EOb1yJJkqThNnDFcnNj36OAZwNUVQGvmKpvVb0eeH2PofYHPtSsdZ5SVf2YKdYzV9Wyru0Du7Z/jmuWJUmSZo2BKpaT7EbnJrtTqmqTb5tsZqMX0pmRliRJkjbJQBXLVfVt4MEtjPPMyW1NAb3LpObXVNUZm3s+SZIkjaaBKpa3pKkK6C1hv9UrZ+I0I29i7QRjc8b6HcZIMJebZuH8uf0OQZI0AGZNsTxTTl++rN8hjIRhfybjIDGXkiRtukF8dJwkSZI0ECyWJUmSpB5chtGyg45Y1u8QRoLrbNtjLjsWzp/LiqVH9TsMSdKQsVhu2arFh/c7BElT8eZbSdImcBmGJEmS1IPFsiRJktSDxbIkSZLUw9AUy0lOSHJIv+OQJEnS7DE0xbIkSZI00waiWE7y5iRHdL0/NskRSd6b5NtJTgfu17V/aZILklyR5D/TsTDJxV19FiW5aJpz7pPk60kuTfKtJDsk2S7J8UkuT3JJksc1ff+k6bM6yWVJ/HVokiRJs8CgPDruv4CTgeVJtgKeBxwNPBXYA7g/8G3gQ03/91bVmwCSfBR4alV9PsmaJIurajVwGHDCVCdLck/gJOC5VXVBknsBE8ARAFW1R5KHAmcm2RV4ObC8qj7WHLt1+ymQtCVNrJ1gfHy8lbHaGkcd5rM95rJd5rM9g57LRYt6z4MORLFcVdcl+UWSPekUxpcABwAfr6rbgRuSnNV1yOOSHA3MAXYCrgQ+DxwHHJbk1cBzgX17nPIhwI+r6oLm/L8BSLI/8O9N21VJfgDsCpwPHJPkgcDJVTXYf+KS7mZszti0X4Ybanx8vJVx1GE+22Mu22U+2zPsuRyIZRiN44AldGaE180g1+ROSbYD3g8cUlV7ACuB7ZrdnwH+nM6M9EVV9Yse58pUYzftd1NV/w08nc7s8xlJHr8B1yNJkqQhN0jF8inAk4F9gDOAc4HnJdk6yQOAxzX91hXGP0+yPXDnEzKq6ubm2A8Ax09zrquAnZPsA9CsV75Hc85Dm7ZdgQcB303yYOCaqloBnAo8rIXrlSRJ0oAbiGUYAFX1uyRnA7+uqtuTnAI8Hrgc+B7w1abfr5OsbNqvAy6YNNTHgL8AzlzPuZ4L/HuSMTozxk+kM2P9wSSXA7cBS6rqlqbvXyW5Ffh/wJvaum5JkiQNroEplpsb+x4FPBugqgp4xVR9q+r1wOt7DLU/8KFmrXNPzXrlR02xa8kUfd8KvHW68SRJkjR6BqJYTrIbcBpwyubcPNfMRi+kMyMtSZIkbZaBKJar6tvAg1sY55mT25oCepdJza+pqjM293ySJEkabQNRLG9JUxXQW9J+q1fO5OlG1sTaCcbmjPU7jJFgLjsWzp/b7xAkSUNo5IvlmXb68mX9DmEkDPszGQeJuZQkadMN0qPjJEmSpIFisSxJkiT14DKMlh10xLJ+hzASXGfbnmHP5cL5c1mx9Kh+hyFJmqUsllu2avHh/Q5BGi3eNCtJ6iOXYUiSJEk9WCxLkiRJPbgMo4cky4CbgHsB51bVV5IcAHwQuBV4dFVN9DFESZIkbWEWy+tRVUu73h4KvLOqju9XPJIkSZo5FstdkhwDvBD4IfAz4KIkJwCnATsCzwGelOSJVXVo3wKVJEnSjLBYbiTZC3gesCedvFwMXLRuf1Udl2R/4LSq+nR/opQkSdJMslj+vQOAU6pqLUCSU/scjyQ6z4keHx/vdxh3MWjxDDvz2R5z2S7z2Z5Bz+WiRYt67rNYvqvqdwCS7mpszti0X2IzbXx8fKDiGXbmsz3msl3msz3DnksfHfd75wLPTDKWZAfgaf0OSJIkSf3lzHKjqi5OchKwGvgBcF6fQ5IkSVKfWSx3qapjgWOn2b9k5qKRJElSv7kMQ5IkSerBYlmSJEnqwWJZkiRJ6sE1yy3bb/XKfocwEibWTjA2Z6zfYYyEYc/lwvlz+x2CJGkWs1hu2enLl/U7hJEw7M9kHCTmUpKkTecyDEmSJKkHi2VJkiSpB5dhtOygI5b1O4SRMOzrbLe0hfPnsmLpUf0OQ5KkkWex3LJViw/vdwiaDbyRVJKkGeEyDEmSJKkHi2VJkiSpB4tlSZIkqYehKZaTLEhyxRTtxyXZbTPGPTDJaZsXnSRJkkbR0N/gV1Uv7XcMkiRJGk1DM7PcuEeSDye5LMmnk8xJck6SvZM8Pcnq5vXdJNf2GiTJk5NcleRrwF90te+b5OtJLml+PqRpPy/J4q5+q5I8bIteqSRJkvpu2GaWHwK8pKpWJfkQ8LfrdlTVqcCpAEk+CXx1qgGSbAesBB4PXA2c1LX7KuCxVXVbkicC/wI8CzgOWAK8KsmuwLZVdVnL1yZtsIm1E4yPj29w/43pq/Uzn+0yn+0xl+0yn+0Z9FwuWrSo575hK5Z/WFWrmu0TgVdO7pDkaGCiqt7XY4yHAtdW1XjT/0Tgr5t984APJ1kEFLBN0/4p4A1JjgJeDJzQwrVIm2xszti0f7G7jY+Pb3BfrZ/5bJf5bI+5bJf5bM+w53LYiuWa7n2SJwDPBh67keOs82bg7Kp6ZpIFwDkAVbU2yZeBZwDPAfbeqKglSZI0lIZtzfKDkjy62f5L4GvrdiT5I+D9wHOqamKaMa4CdkmysGucdeYB1zfbSyYddxywArigqn65aeFLkiRpmAxbsfwd4EVJLgN2Aj7QtW8JMB84pbnJ7wtTDVBVN9NZdnF6c4PfD7p2/yvw1iSrgK0nHXcR8Bvg+JauRZIkSQNuaJZhVNV1wFTPUz6w+Xkh8M8bONaX6Kxdntx+PrBrV9Mb1m0k2ZnOPy7O3KCAJUmSNPSGbWa5L5K8EPgmcExV3dHveCRJkjQzhmZmeVMkOQXYZVLza6rqjI0Zp6o+AnyktcAkSZI0FEa6WK6qZ870OfdbvXKmTzmSJtZOMDZnrN9hDKyF8+f2OwRJkmaFkS6W++H05cv6HcJIGPZnMkqSpNHgmmVJkiSpB4tlSZIkqQeXYbTsoCOW9TuEkTAKa5YXzp/LiqVH9TsMSZK0GSyWW7Zq8eH9DkGDwps9JUkaei7DkCRJknqwWJYkSZJ6sFjuIck5Sfaeon3vJCv6EZMkSZJmlmuWN1JVXQhc2O84JEmStOU5swwkeUOSq5J8OcnHkxzZ7Hp2km8l+V6SA5q+ByY5rY/hSpIkaYbM+mK5WWrxLGBP4C+A7qUX96iqfYFXAW/sQ3iSJEnqI5dhwP7A56pqAiDJ57v2ndz8vAhYMMNxachNrJ1gfHy832EADEwco8J8tstlJ0DaAAAbT0lEQVR8tsdctst8tmfQc7lo0aKe+yyWIdPsu6X5eTvmShtpbM7YtH/5Zsr4+PhAxDEqzGe7zGd7zGW7zGd7hj2Xs34ZBvA14GlJtkuyPXBQvwOSJEnSYJj1s6VVdUGSU4FLgR/QedLFmv5GJUmSpEHgzHLHO6vqIcDBwEOAi6rqwOYxcVTVz6tqQbN9TlU9tX+hSpIkaabM+pnlxn8m2Q3YDvhwVV3c74AkSZLUfxbLQFU9v98xSJIkafC4DEOSJEnqwZnllu23emW/QxgJE2snGJsz1u8wNsvC+XP7HYIkSdpMFsstO335sn6HMBKG/ZmMkiRpNLgMQ5IkSerBYlmSJEnqwWUYLTvoiGX9DmEkDNua5YXz57Ji6VH9DkOSJLXMYrllqxYf3u8Q1A/e2ClJ0khyGYYkSZLUg8WyJEmS1IPF8kZIcnDza7ElSZI0C1gsb5yDAYtlSZKkWWKki+UkC5JcleTDSS5L8ukkj01ycrP/GUkmktwzyXZJrmnaD09yQZJLk3wmyZwkjwGeDrwjyeokC/t5bZIkSdryRrpYbjwE+M+qehjwG2BfYM9m3wHAFcA+wCOBbzbtJ1fVPlX1cOA7wEuq6uvAqcBRVbW4qr4/kxchSZKkmTcbHh33w6pa1WyfCLwSuDrJ/6ZTOL8LeCywNXBe02/3JG8BdgS2B86Y2ZA1bCbWTjA+Pt7vMHoa5NiGkflsl/lsj7lsl/lsz6DnctGiRT33zYZiuaZ4fx7w58CtwFeAE+gUy0c2fU4ADq6qS5MsAQ6cgTg1xMbmjE37F62fxsfHBza2YWQ+22U+22Mu22U+2zPsuZwNyzAelOTRzfZfAl8DzgVeBZxfVT8D5gMPBa5s+u0A/DjJNsChXWPd2OyTJEnSLDAbiuXvAC9KchmwE/ABOmuT70+naAa4DLisqtbNQr+h6fNl4KqusT4BHJXkEm/wkyRJGn2zYRnGHVX18inat123UVV/3b2jqj5Ap6hmUvsqfHScJEnSrDEbZpYlSZKkTTLSM8tVdR2we7/jkCRJ0nByZlmSJEnqYaRnlvthv9Ur+x3CSJhYO8HYnLF+h7HBFs6f2+8QJEnSFmCx3LLTly/rdwgjYdifyShJkkaDyzAkSZKkHiyWJUmSpB5chtGyg45Y1u8QRkI/1iwvnD+XFUuPmtFzSpKkwWax3LJViw/vdwjaVN6cKUmSJnEZhiRJktSDxbIkSZLUg8WyJEmS1MPQFctJliR57xY+x95JVmzJc0iSJGnweYPfFKrqQuDCfschSZKk/hqImeUkRyd5ZbP97iRnNdtPSHJiksOSfC/JV4H9uo57WpJvJrkkyVeS3D/JVknGk9y36bNVkquT3KfHuZ+d5IoklyY5t2k7MMlpzfayJB9Kck6Sa9bFKUmSpNE3KDPL5wL/CKwA9ga2TbINsD8wDvwzsBewBjgbuKQ57mvAo6qqkrwUOLqq/jHJicChwHuAJwKXVtXPe5x7KfCkqro+yY49+jwUeBywA/DdJB+oqls375I1aCbWTjA+Pt7vMLaIUb2ufjGf7TKf7TGX7TKf7Rn0XC5atKjnvkEpli8C9kqyA3ALcDGdovkA4CzgnKr6GUCSk4Bdm+MeCJyU5AHAPYFrm/YPAZ+jUyy/GDh+mnOvAk5I8kng5B59Tq+qW4BbkvwUuD/wo025UA2usTlj0/5lGVbj4+MjeV39Yj7bZT7bYy7bZT7bM+y5HIhlGM0s7XXAYcDXgfPozOQuBL4DVI9D/x14b1XtAbwM2K4Z74fAT5I8Hngk8MVpzv1y4PXAHwKrk8yfotstXdu3Mzj/yJAkSdIWNBDFcuNc4Mjm53nAy4HVwDeAA5PMb5ZmPLvrmHnA9c32iyaNdxxwIvDJqrq910mTLKyqb1bVUuDndIpmSZIkaaCK5fOABwDnV9VPgJuB86rqx8Ay4HzgK3SWaKyzDPhUkvPoFLrdTgW2Z/olGADvSHJ5kivoFOqXbuZ1SJIkaUQMzHKCqvofYJuu97t2bR/PFEVvVX2OztrkqTyczo19V63nvH8xRfM5zYuqWjap/+7TjSdJkqTRMTDFcpuSvBb4GzpPxJAkSZI2yUgWy1X1NuBt3W1JjuGu650BPlVVx85YYJIkSRoqI1ksT6Upird4Ybzf6pVb+hSzwsTaCcbmjM3oORfOnzuj55MkSYNv1hTLM+X05cv6HcJIGPZnMkqSpNEwSE/DkCRJkgaKxbIkSZLUg8swWnbQEcv6HcJQWjh/LiuWHtXvMCRJku7CYrllqxYf3u8QhpM3RkqSpAHkMgxJkiSpB4tlSZIkqQeL5Q2Q5KZ+xyBJkqSZZ7G8HkmCeZIkSZqVLAKnkGRBku8keT9wMTCW5Ngklyb5RpL79ztGSZIkbXkWy709BPhIVe3ZvP9GVT0cOBfwkReSJEmzQKqq3zEMnCQLgLOrapfm/S3AdlVVSZ4L/GlVvXRd/zVr1tyZxB2Pv36Gox0Nj/j6Cv7j6Jeuv6MkSVLLFi1adOf2vHnz0r3P5yz39tuu7Vvr9/+quB3z1rqxOWN3+aCOj4/f5b02nblsl/lsl/lsj7lsl/lsz7Dn0mUYkiRJUg8Wy5IkSVIPLieYQlVdB+ze9X77ru1PA5/uQ1iSJEmaYc4sS5IkST1YLEuSJEk9WCxLkiRJPbhmuWX7rV7Z7xCG0sL5c/sdgiRJ0t1YLLfs9OXL+h2CJEmSWuIyDEmSJKkHi2VJkiSpB5dhtOygI5b1O4RNtnD+XFYsParfYUiSJA0Mi+WWrVp8eL9D2HTenChJknQXLsOQJEmSerBYliRJknqwWJYkSZJ6GJpiOcmSJO/tdxySJEmaPYamWJYkSZJmWl+L5SRHJ3lls/3uJGc1209IcmKSw5J8L8lXgf26jntakm8muSTJV5LcP8lWScaT3Lfps1WSq5Pcp8e575/klCSXNq/HNO2vTnJF83pV0zY3yelNvyuSPHcLp0aSJEkDoN+PjjsX+EdgBbA3sG2SbYD9gXHgn4G9gDXA2cAlzXFfAx5VVZXkpcDRVfWPSU4EDgXeAzwRuLSqft7j3CuAr1bVM5NsDWyfZC/gMOCRQIBvNoX6g4EbquoggCTzWs3CgJhYO8H4+Hi/w7jTIMUy7Mxlu8xnu8xne8xlu8xnewY9l4sWLeq5r9/F8kXAXkl2AG4BLqZTNB8AnAWcU1U/A0hyErBrc9wDgZOSPAC4J3Bt0/4h4HN0iuUXA8dPc+7HAy8EqKrbgTVJ9gdOqarfNuc8uYnlS8A7k7wdOK2qzmvh2gfO2JyxaT8sM2l8fHxgYhl25rJd5rNd5rM95rJd5rM9w57Lvi7DqKpbgevozOZ+HTgPeBywEPgOUD0O/XfgvVW1B/AyYLtmvB8CP0nyeDqzw1/cyJDSI87v0Znhvhx4a5KlGzmuJEmShtAg3OB3LnBk8/M84OXAauAbwIFJ5jdLM57ddcw84Ppm+0WTxjsOOBH4ZDNj3Mv/AH8DkGTrJPdqYjg4yZwkc4FnAucl2RlYW1UnAu8EHrHJVytJkqShMQjF8nnAA4Dzq+onwM3AeVX1Y2AZcD7wFTpLNNZZBnwqyXnA5DXJpwLbM/0SDIAjgMcluZzOcpA/qaqLgROAbwHfBI6rqkuAPYBvJVkNHAO8ZZOuVJIkSUOl32uWqar/Abbper9r1/bxTFH0VtXn6KxNnsrD6dzYd9V6zvsT4BlTtL8LeNektjOAM6YbT5IkSaOn78Vym5K8ls7SikP7HYskSZKG30gVy1X1NuBt3W1JjuGu650BPlVVx85YYJIkSRpKI1UsT6UpimesMN5v9cqZOlXrFs6f2+8QJEmSBsrIF8sz7fTly/odgiRJkloyCE/DkCRJkgaSxbIkSZLUg8swWnbQEcv6HcImWTh/LiuWHtXvMCRJkgaKxXLLVi0+vN8hbJohvjFRkiRpS3EZhiRJktSDxbIkSZLUg8WyJEmS1IPFsiRJktSDxXIjydFJXtlsvzvJWc32E5J8PMkJSa5IcnmSf+hvtJIkSZoJFsu/dy5wQLO9N7B9km2A/YHVwB9U1e5VtQdwfJ9ilCRJ0gxKVfU7hoHQFMbfBR4OnAJcCXwCeDPwRuAjwBeA04Ezq+qOdceuWbPmziTuePz1Mxh1ex7x9RX8x9Ev7XcYkiRJM27RokV3bs+bNy/d+3zOcqOqbk1yHXAY8HXgMuBxwMLm/cOBJwF/BzwHeHF/It0yxuaM3eWD0m/j4+MDFc8wM5ftMp/tMp/tMZftMp/tGfZcWizf1bnAkXQK4cuBdwEXAfOB31XVZ5J8HzihbxFKkiRpxlgs39V5wDHA+VX12yQ3N21/AByfZN0a79f1K0BJkiTNHIvlLlX1P8A2Xe937dr9iJmPSJIkSf3k0zAkSZKkHiyWJUmSpB4sliVJkqQeXLPcsv1Wr+x3CJtk4fy5/Q5BkiRp4Fgst+z05cv6HYIkSZJa4jIMSZIkqQeLZUmSJKkHl2G07KAjlvU7hGktnD+XFUuP6ncYkiRJQ8FiuWWrFh/e7xCmN6Q3IEqSJPWDyzAkSZKkHiyWJUmSpB4sliVJkqQeLJbXI8mSJDt3vT8uyW79jEmSJEkzwxv81m8JcAVwA0BVvbSv0UiSJGnGWCx3SfIG4FDgh8DPgYuAvYGPJZkAHg18ETiyqi7sW6CSJEmaERbLjSR7A88C9qSTl4vpFMsX0lUcJ+lbjG2YWDvB+Ph4v8PYIMMS5zAwl+0yn+0yn+0xl+0yn+0Z9FwuWrSo5z6L5d/bH/hcVU0AJPl8n+PZIsbmjE37gRgU4+PjQxHnMDCX7TKf7TKf7TGX7TKf7Rn2XHqD3+8N95SxJEmSWmex/HtfA56WZLsk2wMHNe03Ajv0LyxJkiT1i8swGlV1QZJTgUuBH9BZq7wGOAH4YNcNfpIkSZolLJbv6p1VtSzJHOBc4N+q6mLgM119DuxLZJIkSZpxFst39Z/NLxzZDvhwUyhLkiRplrJY7lJVz+93DJIkSRocFsst22/1yn6HMK2F8+f2OwRJkqShYbHcstOXL+t3CJIkSWqJj46TJEmSerBYliRJknqwWJYkSZJ6sFiWJEmSerBYliRJknqwWJYkSZJ6sFiWJEmSerBYliRJknqwWJYkSZJ6sFiWJEmSerBYliRJknqwWJYkSZJ6SFX1O4aht2bNGpMoSZI0AubNm5fu984sS5IkST1YLEuSJEk9uAxDkiRJ6sGZZUmSJKkHi+Uekjw5yXeTXJ3ktVPs3zbJSc3+byZZ0LXvdU37d5M8aUPHHFWbmsskf5rkoiSXNz8f33XMOc2Yq5vX/WbuivprM/K5IMlEV84+2HXMXk2er06yIkkmjzuKNiOXh3blcXWSO5Isbvb52eydz8cmuTjJbUkOmbTvRUnGm9eLutpn5WcTNj2fSRYnOT/JlUkuS/Lcrn0nJLm26/O5eKaup58287N5e1e+Tu1q36X5XhhvvifuORPXMgg247P5uEnfnTcnObjZN7ifzaryNekFbA18H3gwcE/gUmC3SX3+Fvhgs/084KRme7em/7bALs04W2/ImKP42sxc7gns3GzvDlzfdcw5wN79vr4hy+cC4Ioe434LeDQQ4IvAn/f7Wgc5l5P67AFc0/Xez2bvfC4AHgZ8BDikq30n4Jrm572b7Xs3+2bdZ7OFfO4KLGq2dwZ+DOzYvD+hu+9seG1OLpt9N/UY95PA85rtDwJ/0+9rHYZ8dvXZCfglMKd5P7CfTWeWp7YvcHVVXVNVvwM+ATxjUp9nAB9utj8NPKGZ8XgG8ImquqWqrgWubsbbkDFH0SbnsqouqaobmvYrge2SbDsjUQ+uzflsTinJA4B7VdX51fnG+ghwcPuhD5y2cvmXwMe3aKTDYb35rKrrquoy4I5Jxz4J+HJV/bKqfgV8GXjyLP5swmbks6q+V1XjzfYNwE+B+85M2ANpcz6bU2q+Bx5P53sBOt8TfjYbG5jPQ4AvVtXaLRdqOyyWp/YHwA+73v+oaZuyT1XdBqwB5k9z7IaMOYo2J5fdngVcUlW3dLUd3/yvmjfMov81u7n53CXJJUm+muSArv4/Ws+Yo6itz+ZzuXux7Gdz4z5H031vzsbPJrT034wk+9KZ/ft+V/OxzfKMd8+SCYjNzeV2SS5M8o11SwbofA/8uvle2JQxh1lb9czzuPt350B+Ni2WpzbVf9wmPzakV5+NbR91m5PLzs7kT4C3Ay/r2n9oVe0BHNC8XrCZcQ6Lzcnnj4EHVdWewKuB/05yrw0ccxS18dl8JLC2qq7o2u9n8/c29HPk9+bdbfa1NzPzHwUOq6p1M3yvAx4K7EPnf4O/ZnOCHBKbm8sHVdXewPOB9yRZ2MKYw6ytz+YewBldzQP72bRYntqPgD/sev9A4IZefZLcA5hHZ+1Nr2M3ZMxRtDm5JMkDgVOAF1bVnTMjVXV98/NG4L/p/G+h2WCT89ksDfoFQFVdRGemadem/wPXM+Yo2qzPZuNuMyN+Nu+0MZ+j6b43Z+NnEzbzvxnNP4RPB15fVd9Y115VP66OW4DjmR2fz83K5brlgFV1DZ17EvYEfg7s2HwvbPSYQ66NeuY5wClVdeu6hkH+bFosT+0CYFFzp+s96fwH8dRJfU4F1t2xfQhwVrOm7lTgeencRb8LsIjODSobMuYo2uRcJtmRzpf966pq1brOSe6R5D7N9jbAU4ErmB02J5/3TbI1QJIH0/lsXlNVPwZuTPKoZsnAC4HPzcTF9Nnm/D0nyVbAs+ms16Np87O5ad9xZwB/luTeSe4N/Blwxiz+bMJm5LPpfwrwkar61KR9D2h+hs4a29nw+dycXN573XKA5u/2fsC3m++Bs+l8L0Dne8LP5oa7270eA/3Z7PcdhoP6Ap4CfI/O7NsxTdubgKc329sBn6JzA9+3gAd3HXtMc9x36bpze6oxZ8NrU3MJvB74LbC663U/YC5wEXAZnRv/lgNb9/s6hyCfz2rydSlwMfC0rjH3pvPF9H3gvTS/sGjUX5v59/xA4BuTxvOzOX0+96EzK/Vb4BfAlV3HvrjJ89V0lg3M6s/m5uQT+Cvg1knfnYubfWcBlzc5PRHYvt/XOeC5fEyTr0ubny/pGvPBzffC1c33xLb9vs5Bz2ezbwFwPbDVpDEH9rPpb/CTJEmSenAZhiRJktSDxbIkSZLUg8WyJEmS1IPFsiRJktSDxbIkSZLUg8WyJA2ZJMuSnNhsPyjJTeueod3iOa5L8sQ2x5SkYWSxLEmTNIXiT5LM7Wp7aZJz+hjWlKrq/1bV9lV1+0ydM8kJSd4yU+ebTvc/HCRpS7BYlqSp3QM4YnMHSYfftVtA168alqQtxi9wSZraO4Ajm1+7fjdJHpPkgiRrmp+P6dp3TpJjk6wC1gIPbtrekuTrzbKJzyeZn+RjSX7TjLGga4zlSX7Y7LsoyQE94liQpJpftf3oZux1r5uTXNf02yrJa5N8P8kvknwyyU5d47wgyQ+afcdsaJK6zn9YE++vkrw8yT5JLkvy6yTv7eq/JMmqJP/e5O6qJE/o2r9zklOT/DLJ1UkO79q3LMmnk5yY5DfAy4F/Ap7bXO+lTb/DknwnyY1Jrknysq4xDkzyoyT/mOSnSX6c5LCu/WNJ/q3JxZokX0sy1ux7VPPn9+sklyY5cEPzJGl4WSxL0tQuBM4Bjpy8oykyTwdWAPOBdwGnJ5nf1e0FwF8DOwA/aNqe17T/AbAQOB84HtgJ+A7wxq7jLwAWN/v+G/hUku2mC7iqzm+WZGwP3Bv4BvDxZvcrgYOB/wPsDPwKeF9zPbsBH2hi27m5pgdOd64pPBJYBDwXeA9wDPBE4E+A5yT5P5P6XgPcp7nmk7sK94/T+TW5OwOHAP/SXUwDzwA+DewI/BfwL8BJzXU/vOnzU+CpwL2Aw4B3J3lE1xj/C5hH58/hJcD7kty72fdOYC86v+Z4J+Bo4I4kf0Dnz/wtTfuRwGeS3Hcj8yRpyFgsS1JvS4G/n6IgOggYr6qPVtVtVfVx4CrgaV19TqiqK5v9tzZtx1fV96tqDfBF4PtV9ZWqug34FLDnuoOr6sSq+kVz/L8B2wIP2YjYVwC/pVO0ArwMOKaqflRVtwDLgEOapQyHAKdV1bnNvjcAd2zEuQDeXFU3V9WZzXk/XlU/rarrgfO6r41OMfueqrq1qk4CvgsclOQP/3879w9aZxXGcfz7+KcNaqFKtVix6aTo4OqSQRCUSCWLVQwiIjoJdfBPnSyklOoiuDmIizpUKxXRC9bJpeoighah6FBiWkqSEqnagq2Pwzlv++Zy35vbkiCR7wdC8ubcnPeck+E+9/A7LzAB7Kl9/QC8SyniG99k5qeZ+U9mnhs0kMz8oq5zZubXwBGgvTP/NzBT798D/gDurnGZZ4EXM3MuMy9m5tG6Jk8Bvczs1Xt/RflA9cgVrpOkdcZiWZI6ZOZPwOfAa31N27i8W9w4QdmpbMwO6PJ06+dzA65vai5qTODnGgVYouyEbhll3DV28AAwnZlN0TsOHK4RgiXKTvZFYGudz6XxZuafwOIo97qauQFzmZmt6xN1DNuAM5l5tq9tpXVdJiImI+LbGuVYohS07bVbrB9QGn/V8W0BxoBfB3Q7Duxq1q/2OwHcvtJ4JK1vFsuSNNxe4HmWF2wnKcVT23ZgrnWdXKWaT94DPA7cnJmbgd+BGPFv9wFTdQe7MQtMZubm1tdY3fk9BdzZ6uMGShRjrdwREe25bKes6UnglojY1Nc2bF2XXUfERuATSpxia127HiOsHbAAnKdEZPrNAu/3rd+NmfnGCP1KWscsliVpiMz8BThIyfw2esBdETFdD9Y9AdxL2YVeDZuAC8A8cF1EvE7J3w5VYwwHgacz83hf8zvA/ogYr6+9NSKmatshYGdETETEBmCGtX1/uA3YHRHXR8Qu4B5KxGEWOAociIixiLiPkin+cEhfp4EdcfmJIxsokZV54EJETAIPjTKougv/HvBWPWh4bT00uRH4AHg0Ih6uvx+rhwWvNNstaZ2xWJaklc0Al565nJmLlANkL1HiCq8COzNzYZXu9yUl03ycEkM4zwjxA+BByuG1Q60nYhyrbW8DnwFHIuIs5fDf/XU+x4AXKAcJT1EO//22SnMZ5DvKYcAFYD/wWF1TgCeBHZRd5sPA3poP7vJx/b4YEd/XCMdu4CPKPKYp8x7Vy8CPlAOWZ4A3gWtqIT9FefrGPOX/8Qq+j0r/e7E8NiZJ0tqJiGeA5zJz4r8eiySNwk/EkiRJUgeLZUmSJKmDMQxJkiSpgzvLkiRJUgeLZUmSJKmDxbIkSZLUwWJZkiRJ6mCxLEmSJHWwWJYkSZI6/As4641Q0cnxYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x249f108ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine Modeling Function\n",
    "\n",
    "The next step is to refactor all of the indvidual code we walked through into a single function. This function will create the model, perform cross validation to find the optimal number of iterations, train the model on the whole training dataset using the optimal number of iterations, make predictions on the test data, and evaluate the MAPE on the test set. In order to integrate with the other models implemented in the modeling notebook, this function needs to return a numpy array:\n",
    "\n",
    "`['model', train_time, test_time, test_mape]`\n",
    "\n",
    "It also must take in the same arguments: a set of training features, training targets, a set of testing features, testing targets. We will suppress the messages returned during training for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_model(train, targets, test, test_targets, n_folds = 5):\n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation to select the optimal number of training iterations. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        train : dataframe, shape = [n_training_samples, n_features]\n",
    "            Set of training features for training a model\n",
    "    \n",
    "        targets : array, shape = [n_training_samples]\n",
    "            Array of training targets for training a model\n",
    "\n",
    "        test : dataframe, shape = [n_testing_samples, n_features]\n",
    "            Set of testing features for making predictions with a model\n",
    "\n",
    "        test_targets : array, shape = [n_testing_samples]\n",
    "            Array of testing targets for evaluating the model predictions\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        results : array, shape = [4]\n",
    "            Numpy array of results. \n",
    "            First entry is the model, second is the training time,\n",
    "            third is the testing time, and fourth is the MAPE. All entries\n",
    "            are in strings and so will need to be converted to numbers.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # KFold cross validation object\n",
    "    kfold = KFold(n_splits = n_folds)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    \n",
    "    best_iterations = 0\n",
    "    \n",
    "    # Create the model with specified hyperparaters\n",
    "    model = lgb.LGBMRegressor(n_estimators=10000,\n",
    "                              learning_rate = 0.01, \n",
    "                              reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                              subsample = 0.9, n_jobs = -1)\n",
    "    \n",
    "    # Cross validation to find optimal number of iterations\n",
    "    for train_indices, valid_indices in kfold.split(train):\n",
    "        \n",
    "        # Training data for fold\n",
    "        train_features, train_targets = np.array(train)[train_indices], targets[train_indices]\n",
    "    \n",
    "        # Validation data for fold\n",
    "        valid_features, valid_targets = np.array(train)[valid_indices], targets[valid_indices]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X = train_features, y = train_targets, \n",
    "                  early_stopping_rounds = 100,\n",
    "                  eval_metric = mape,\n",
    "                  eval_set = [(valid_features, valid_targets)],\n",
    "                  eval_names = ['valid'], verbose = False)\n",
    "    \n",
    "        # Add the number of iterations to the total for averaging\n",
    "        best_iterations += model.best_iteration_\n",
    "        \n",
    "        \n",
    "    # Average the best iterations across folds\n",
    "    best_iterations = int(best_iterations / kfold.n_splits)\n",
    "    \n",
    "    # Create the model with optimal number of iterations\n",
    "    model = lgb.LGBMRegressor(n_estimators=iterations,\n",
    "                              learning_rate = 0.01, \n",
    "                              reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                              subsample = 0.9, n_jobs = -1)\n",
    "    \n",
    "    # Start the training time\n",
    "    start = timer()\n",
    "    \n",
    "    # Fit on the entire training set\n",
    "    model.fit(train, targets, verbose = False)\n",
    "    \n",
    "    # End the training time\n",
    "    end = timer()\n",
    "    train_time = end - start\n",
    "    \n",
    "    # Start the testing time\n",
    "    start = timer()\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    predictions = model.predict(test)\n",
    "    \n",
    "    # End the testing time\n",
    "    end = timer()\n",
    "    test_time = end - start\n",
    "    \n",
    "    # Calculate the mape\n",
    "    _, test_mape, _ = mape(test_targets, predictions)\n",
    "    \n",
    "    \n",
    "    # Record the results and return\n",
    "    results = np.array(['gbm', train_time, test_time, test_mape])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gbm', '6.21472920343108', '0.31054537055683884',\n",
       "       '16.44666645768823'], dtype='<U19')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the model\n",
    "gbm_results = gbm_model(train, targets, test, test_targets)\n",
    "gbm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface with the other Models\n",
    "\n",
    "When we evaluate the models, it will be simplest to do all at once. Therefore, we need to fit this model into our existing functions. The return value is the same, so we can call the model function and add the results to the rest. \n",
    "\n",
    "To integrate this function with the six previous models already developed, we can simply add in the function call to the `evaluate_models` function we already wrote. This is the best part about writing functions with standard outputs and inputs: everything can work together! Writing functions increases efficiency and makes reproducing results much easier than ad hoc code scattered around a notebook (speaking from experience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(df):\n",
    "    \"\"\"Evaluate machine learning models\n",
    "    on a building energy dataset. More models can be added\n",
    "    to the function as required. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Building energy dataframe. Each row must have one observation\n",
    "        and the columns must contain the features. The dataframe\n",
    "        needs to have an \"elec_cons\" column to be used as targets. \n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    results : dataframe, shape = [n_models, 4]\n",
    "        Modeling metrics. A dataframe with columns:\n",
    "        model, train_time, test_time, mape. Used for comparing\n",
    "        models for a given building dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the data for machine learning\n",
    "        train, train_targets, test, test_targets = preprocess_data(df, test_days = 183, scale = True)\n",
    "    except Exception as e:\n",
    "        print('Error processing data: ', e)\n",
    "        return\n",
    "        \n",
    "    # elasticnet\n",
    "    model = ElasticNet(alpha = 1.0, l1_ratio=0.5)\n",
    "    elasticnet_results = implement_model(model, train, train_targets, test, \n",
    "                                         test_targets, model_name = 'elasticnet')\n",
    "    \n",
    "    # knn\n",
    "    model = KNeighborsRegressor()\n",
    "    knn_results = implement_model(model, train, train_targets, test, \n",
    "                                  test_targets, model_name = 'knn')\n",
    "    \n",
    "    # svm\n",
    "    model = SVR()\n",
    "    svm_results = implement_model(model, train, train_targets, test, \n",
    "                                   test_targets, model_name = 'svm')\n",
    "    \n",
    "    # rf\n",
    "    model = RandomForestRegressor(n_estimators = 100, n_jobs = -1)\n",
    "    rf_results = implement_model(model, train, train_targets, test, \n",
    "                                  test_targets, model_name = 'rf')\n",
    "    \n",
    "    # et\n",
    "    model = ExtraTreesRegressor(n_estimators=100, n_jobs = -1)\n",
    "    et_results = implement_model(model, train, train_targets, test, \n",
    "                                  test_targets, model_name = 'et')\n",
    "    \n",
    "    # adaboost\n",
    "    model = AdaBoostRegressor(n_estimators = 1000, learning_rate = 0.05, \n",
    "                              loss = 'exponential')\n",
    "    adaboost_results = implement_model(model, train, train_targets, test, \n",
    "                                       test_targets, model_name = 'adaboost')\n",
    "    \n",
    "    # gbm\n",
    "    gbm_results = gbm_model(train, train_targets, test, test_targets)\n",
    "    \n",
    "    # Put the results into a single array (stack the rows)\n",
    "    results = np.vstack((elasticnet_results, knn_results, svm_results,\n",
    "                         rf_results, et_results, gbm_results, adaboost_results))\n",
    "    \n",
    "    # Convert the results to a dataframe\n",
    "    results = pd.DataFrame(results, columns = ['model', 'train_time', 'test_time', 'mape'])\n",
    "    \n",
    "    # Convert the numeric results to numbers\n",
    "    results.iloc[:, 1:] = results.iloc[:, 1:].astype(np.float32)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_time</th>\n",
       "      <th>test_time</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.0765513</td>\n",
       "      <td>0.00460913</td>\n",
       "      <td>56.9737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>54.2147</td>\n",
       "      <td>3.23389</td>\n",
       "      <td>23.7799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>674.615</td>\n",
       "      <td>55.4858</td>\n",
       "      <td>24.8263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>28.751</td>\n",
       "      <td>0.219195</td>\n",
       "      <td>16.1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>et</td>\n",
       "      <td>14.8552</td>\n",
       "      <td>0.208349</td>\n",
       "      <td>18.0844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gbm</td>\n",
       "      <td>6.95854</td>\n",
       "      <td>0.338123</td>\n",
       "      <td>16.4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>280.147</td>\n",
       "      <td>2.07414</td>\n",
       "      <td>38.9557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model train_time   test_time     mape\n",
       "0  elasticnet  0.0765513  0.00460913  56.9737\n",
       "1         knn    54.2147     3.23389  23.7799\n",
       "2         svm    674.615     55.4858  24.8263\n",
       "3          rf     28.751    0.219195  16.1971\n",
       "4          et    14.8552    0.208349  18.0844\n",
       "5         gbm    6.95854    0.338123  16.4467\n",
       "6    adaboost    280.147     2.07414  38.9557"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/f-APS_weather.csv')\n",
    "model_results = evaluate_models(df)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_time</th>\n",
       "      <th>test_time</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.00777841</td>\n",
       "      <td>0.00189042</td>\n",
       "      <td>52.3209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.0735118</td>\n",
       "      <td>4.14259</td>\n",
       "      <td>27.4117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>10.1074</td>\n",
       "      <td>8.33936</td>\n",
       "      <td>38.2993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>2.51328</td>\n",
       "      <td>0.104967</td>\n",
       "      <td>21.6745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>et</td>\n",
       "      <td>1.40114</td>\n",
       "      <td>0.105561</td>\n",
       "      <td>24.4212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gbm</td>\n",
       "      <td>2.54485</td>\n",
       "      <td>0.37309</td>\n",
       "      <td>21.8775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>27.7201</td>\n",
       "      <td>1.90376</td>\n",
       "      <td>26.7783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  train_time   test_time     mape\n",
       "0  elasticnet  0.00777841  0.00189042  52.3209\n",
       "1         knn   0.0735118     4.14259  27.4117\n",
       "2         svm     10.1074     8.33936  38.2993\n",
       "3          rf     2.51328    0.104967  21.6745\n",
       "4          et     1.40114    0.105561  24.4212\n",
       "5         gbm     2.54485     0.37309  21.8775\n",
       "6    adaboost     27.7201     1.90376  26.7783"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv('../data/f-Kansas_weather.csv')\n",
    "new_model_results = evaluate_models(new_df)\n",
    "new_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid reading too much into the metrics at this point, but the Gradient Boosting Machine looks as if it may compete. The difficult part of determining the real \"best\" model is the influence of the hyperparameters. The best model/settings probably is different for every building and optimizing all the models would require an extensive search procedure. For now we will have to be content with selecting a set of hyperparameters and using those for the model on each building. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The [gradient boosting machine](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) is a powerful boosting ensemble machine learning model. While it has many hyperparameters to tune, the high performance more than makes up for the development time. In this notebook, we walked through am implementation of the gradient boosting machine for the building energy prediction task. The model uses cross validation to find the optimal number of base learners and is built using a sensible set of additional hyperparameters. We stepped through the process of building the model, tuning the number of iterations using cross validation, fitting the optimal model, making predictions with the model, and evaluating the model predictions. Preliminary results from two buildings suggest the Gradient Boosting Machine is competitive with the bagging, tree-based ensemble models in Scikit-Learn although more data is needed to draw significant conclusions.\n",
    "\n",
    "Furthermore, in this notebook we saw how we can use feature importances to try and understand how the model makes predictions. Model interpretation will be a focus of a later notebook, but this is one technique we may be able to employ ([Local-Interpretable Model Agnostic Explanations (LIME)](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) is another tool). The next step is to evaluate all of the models developed on hundreds of buildings to gather enough data to make significant conclusions. After evaluation, the best model can be selected for optimization through hyperparameter tuning. Then, we can make efforts to try and explain the predictions of the model. It's one thing to know a machine learning model can make accurate predictions but another problem entirely to understand how the model makes predictions. I will see you in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
